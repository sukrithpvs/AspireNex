{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4b81c9-c9d3-46c3-90ef-ece2fbd28cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68833e4-4fbb-4883-98b9-985107e2ceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Path: Images/1000268201_693b08cb0e.jpg\n",
      "Caption: image,caption\n",
      "\n",
      "Image Path: Images/1001773457_577c3a7d70.jpg\n",
      "Caption: 1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "\n",
      "Image Path: Images/1002674143_1b742ab4b8.jpg\n",
      "Caption: 1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
      "\n",
      "Image Path: Images/1003163366_44323f5815.jpg\n",
      "Caption: 1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
      "\n",
      "Image Path: Images/1007129816_e794419615.jpg\n",
      "Caption: 1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\n",
      "\n",
      "Image Path: Images/1007320043_627395c3d8.jpg\n",
      "Caption: 1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "images_folder = r\"Images\"\n",
    "captions_file = r\"captions.txt\"\n",
    "\n",
    "# Read captions from the file\n",
    "with open(captions_file, 'r') as file:\n",
    "    captions = file.readlines()\n",
    "\n",
    "# Display first few image paths and captions\n",
    "for i, filename in enumerate(os.listdir(images_folder)):\n",
    "    image_path = os.path.join(images_folder, filename)\n",
    "    caption = captions[i].strip() if i < len(captions) else \"No caption available\"\n",
    "    print(f\"Image Path: {image_path}\")\n",
    "    print(f\"Caption: {caption}\")\n",
    "    print()\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87837399-1f94-4817-87e0-97a243b53999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31fde0bf-0bbe-41fb-8efe-ec726cab18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define Flickr8kDataset\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, tokenizer, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_captions = pd.read_csv(annotations_file)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_captions)//5\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.img_captions.iloc[5*idx, 0]\n",
    "        img_path = os.path.join(self.img_dir, file_name)\n",
    "        image = Image.open(img_path)\n",
    "        caption = random.choice(self.img_captions.iloc[5*idx : 5*(idx+1), 1].tolist())\n",
    "        tokenized_caption = self.tokenizer.encode(caption)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, tokenized_caption\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "dataset = Flickr8kDataset(annotations_file=captions_file, img_dir=images_folder, tokenizer=tokenizer, transform=transform)\n",
    "split_point = int(0.9*len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [split_point, len(dataset) - split_point])\n",
    "\n",
    "def collate_fn(data):\n",
    "    images, captions = zip(*data)\n",
    "    images = torch.stack(images, 0)\n",
    "    captions = [[tokenizer.bos_token_id] + cap + [tokenizer.eos_token_id] for cap in captions]\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths), dtype=torch.long)\n",
    "    masks = torch.zeros(len(captions), max(lengths), dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = torch.LongTensor(cap)\n",
    "        masks[i, :end] = 1\n",
    "    return images, targets, masks\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1fe8e0-4b78-47a9-860c-93acd0d5fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Iteration 50/228 | Training Loss: 4.220\n",
      "Epoch 1/25 | Iteration 100/228 | Training Loss: 2.326\n",
      "Epoch 1/25 | Iteration 150/228 | Training Loss: 1.811\n",
      "Epoch 1/25 | Iteration 200/228 | Training Loss: 1.670\n",
      "Epoch 1/25 | Training Loss: 2.402 | Validation Loss: 1.610\n",
      "\n",
      "Epoch 2/25 | Iteration 50/228 | Training Loss: 1.592\n",
      "Epoch 2/25 | Iteration 100/228 | Training Loss: 1.600\n",
      "Epoch 2/25 | Iteration 150/228 | Training Loss: 1.625\n",
      "Epoch 2/25 | Iteration 200/228 | Training Loss: 1.537\n",
      "Epoch 2/25 | Training Loss: 1.586 | Validation Loss: 1.561\n",
      "\n",
      "Epoch 3/25 | Iteration 50/228 | Training Loss: 1.598\n",
      "Epoch 3/25 | Iteration 100/228 | Training Loss: 1.506\n",
      "Epoch 3/25 | Iteration 150/228 | Training Loss: 1.557\n",
      "Epoch 3/25 | Iteration 200/228 | Training Loss: 1.544\n",
      "Epoch 3/25 | Training Loss: 1.544 | Validation Loss: 1.476\n",
      "\n",
      "Epoch 4/25 | Iteration 50/228 | Training Loss: 1.497\n",
      "Epoch 4/25 | Iteration 100/228 | Training Loss: 1.488\n",
      "Epoch 4/25 | Iteration 150/228 | Training Loss: 1.488\n",
      "Epoch 4/25 | Iteration 200/228 | Training Loss: 1.474\n",
      "Epoch 4/25 | Training Loss: 1.491 | Validation Loss: 1.474\n",
      "\n",
      "Epoch 5/25 | Iteration 50/228 | Training Loss: 1.415\n",
      "Epoch 5/25 | Iteration 100/228 | Training Loss: 1.434\n",
      "Epoch 5/25 | Iteration 150/228 | Training Loss: 1.437\n",
      "Epoch 5/25 | Iteration 200/228 | Training Loss: 1.440\n",
      "Epoch 5/25 | Training Loss: 1.432 | Validation Loss: 1.481\n",
      "\n",
      "Epoch 6/25 | Iteration 50/228 | Training Loss: 1.425\n",
      "Epoch 6/25 | Iteration 100/228 | Training Loss: 1.565\n",
      "Epoch 6/25 | Iteration 150/228 | Training Loss: 1.581\n",
      "Epoch 6/25 | Iteration 200/228 | Training Loss: 1.493\n",
      "Epoch 6/25 | Training Loss: 1.504 | Validation Loss: 1.427\n",
      "\n",
      "Epoch 7/25 | Iteration 50/228 | Training Loss: 1.414\n",
      "Epoch 7/25 | Iteration 100/228 | Training Loss: 1.409\n",
      "Epoch 7/25 | Iteration 150/228 | Training Loss: 1.382\n",
      "Epoch 7/25 | Iteration 200/228 | Training Loss: 1.321\n",
      "Epoch 7/25 | Training Loss: 1.385 | Validation Loss: 1.395\n",
      "\n",
      "Epoch 8/25 | Iteration 50/228 | Training Loss: 1.369\n",
      "Epoch 8/25 | Iteration 100/228 | Training Loss: 1.340\n",
      "Epoch 8/25 | Iteration 150/228 | Training Loss: 1.356\n",
      "Epoch 8/25 | Iteration 200/228 | Training Loss: 1.357\n",
      "Epoch 8/25 | Training Loss: 1.359 | Validation Loss: 1.443\n",
      "\n",
      "Epoch 9/25 | Iteration 50/228 | Training Loss: 1.353\n",
      "Epoch 9/25 | Iteration 100/228 | Training Loss: 1.308\n",
      "Epoch 9/25 | Iteration 150/228 | Training Loss: 1.295\n",
      "Epoch 9/25 | Iteration 200/228 | Training Loss: 1.297\n",
      "Epoch 9/25 | Training Loss: 1.311 | Validation Loss: 1.483\n",
      "\n",
      "Epoch 10/25 | Iteration 50/228 | Training Loss: 1.243\n",
      "Epoch 10/25 | Iteration 100/228 | Training Loss: 1.306\n",
      "Epoch 10/25 | Iteration 150/228 | Training Loss: 1.303\n",
      "Epoch 10/25 | Iteration 200/228 | Training Loss: 1.292\n",
      "Epoch 10/25 | Training Loss: 1.282 | Validation Loss: 1.449\n",
      "\n",
      "Epoch 11/25 | Iteration 50/228 | Training Loss: 1.267\n",
      "Epoch 11/25 | Iteration 100/228 | Training Loss: 1.290\n",
      "Epoch 11/25 | Iteration 150/228 | Training Loss: 1.239\n",
      "Epoch 11/25 | Iteration 200/228 | Training Loss: 1.281\n",
      "Epoch 11/25 | Training Loss: 1.267 | Validation Loss: 1.448\n",
      "\n",
      "Epoch 12/25 | Iteration 50/228 | Training Loss: 1.238\n",
      "Epoch 12/25 | Iteration 100/228 | Training Loss: 1.245\n",
      "Epoch 12/25 | Iteration 150/228 | Training Loss: 1.235\n",
      "Epoch 12/25 | Iteration 200/228 | Training Loss: 1.289\n",
      "Epoch 12/25 | Training Loss: 1.255 | Validation Loss: 1.400\n",
      "\n",
      "Epoch 13/25 | Iteration 50/228 | Training Loss: 1.241\n",
      "Epoch 13/25 | Iteration 100/228 | Training Loss: 1.282\n",
      "Epoch 13/25 | Iteration 150/228 | Training Loss: 1.221\n",
      "Epoch 13/25 | Iteration 200/228 | Training Loss: 1.216\n",
      "Epoch 13/25 | Training Loss: 1.240 | Validation Loss: 1.366\n",
      "\n",
      "Epoch 14/25 | Iteration 50/228 | Training Loss: 1.211\n",
      "Epoch 14/25 | Iteration 100/228 | Training Loss: 1.205\n",
      "Epoch 14/25 | Iteration 150/228 | Training Loss: 1.237\n",
      "Epoch 14/25 | Iteration 200/228 | Training Loss: 1.238\n",
      "Epoch 14/25 | Training Loss: 1.227 | Validation Loss: 1.446\n",
      "\n",
      "Epoch 15/25 | Iteration 50/228 | Training Loss: 1.237\n",
      "Epoch 15/25 | Iteration 100/228 | Training Loss: 1.176\n",
      "Epoch 15/25 | Iteration 150/228 | Training Loss: 1.201\n",
      "Epoch 15/25 | Iteration 200/228 | Training Loss: 1.192\n",
      "Epoch 15/25 | Training Loss: 1.205 | Validation Loss: 1.383\n",
      "\n",
      "Epoch 16/25 | Iteration 50/228 | Training Loss: 1.188\n",
      "Epoch 16/25 | Iteration 100/228 | Training Loss: 1.208\n",
      "Epoch 16/25 | Iteration 150/228 | Training Loss: 1.218\n",
      "Epoch 16/25 | Iteration 200/228 | Training Loss: 1.164\n",
      "Epoch 16/25 | Training Loss: 1.196 | Validation Loss: 1.366\n",
      "\n",
      "Epoch 17/25 | Iteration 50/228 | Training Loss: 1.165\n",
      "Epoch 17/25 | Iteration 100/228 | Training Loss: 1.194\n",
      "Epoch 17/25 | Iteration 150/228 | Training Loss: 1.188\n",
      "Epoch 17/25 | Iteration 200/228 | Training Loss: 1.158\n",
      "Epoch 17/25 | Training Loss: 1.173 | Validation Loss: 1.453\n",
      "\n",
      "Epoch 18/25 | Iteration 50/228 | Training Loss: 1.119\n",
      "Epoch 18/25 | Iteration 100/228 | Training Loss: 1.130\n",
      "Epoch 18/25 | Iteration 150/228 | Training Loss: 1.130\n",
      "Epoch 18/25 | Iteration 200/228 | Training Loss: 1.190\n",
      "Epoch 18/25 | Training Loss: 1.144 | Validation Loss: 1.422\n",
      "\n",
      "Epoch 19/25 | Iteration 50/228 | Training Loss: 1.095\n",
      "Epoch 19/25 | Iteration 100/228 | Training Loss: 1.111\n",
      "Epoch 19/25 | Iteration 150/228 | Training Loss: 1.134\n",
      "Epoch 19/25 | Iteration 200/228 | Training Loss: 1.139\n",
      "Epoch 19/25 | Training Loss: 1.123 | Validation Loss: 1.356\n",
      "\n",
      "Epoch 20/25 | Iteration 50/228 | Training Loss: 1.113\n",
      "Epoch 20/25 | Iteration 100/228 | Training Loss: 1.093\n",
      "Epoch 20/25 | Iteration 150/228 | Training Loss: 1.108\n",
      "Epoch 20/25 | Iteration 200/228 | Training Loss: 1.108\n",
      "Epoch 20/25 | Training Loss: 1.110 | Validation Loss: 1.398\n",
      "\n",
      "Epoch 21/25 | Iteration 50/228 | Training Loss: 1.089\n",
      "Epoch 21/25 | Iteration 100/228 | Training Loss: 1.089\n",
      "Epoch 21/25 | Iteration 150/228 | Training Loss: 1.081\n",
      "Epoch 21/25 | Iteration 200/228 | Training Loss: 1.130\n",
      "Epoch 21/25 | Training Loss: 1.096 | Validation Loss: 1.399\n",
      "\n",
      "Epoch 22/25 | Iteration 50/228 | Training Loss: 1.082\n",
      "Epoch 22/25 | Iteration 100/228 | Training Loss: 1.074\n",
      "Epoch 22/25 | Iteration 150/228 | Training Loss: 1.060\n",
      "Epoch 22/25 | Iteration 200/228 | Training Loss: 1.095\n",
      "Epoch 22/25 | Training Loss: 1.078 | Validation Loss: 1.343\n",
      "\n",
      "Epoch 23/25 | Iteration 50/228 | Training Loss: 1.067\n",
      "Epoch 23/25 | Iteration 100/228 | Training Loss: 1.087\n",
      "Epoch 23/25 | Iteration 150/228 | Training Loss: 1.106\n",
      "Epoch 23/25 | Iteration 200/228 | Training Loss: 1.085\n",
      "Epoch 23/25 | Training Loss: 1.081 | Validation Loss: 1.396\n",
      "\n",
      "Epoch 24/25 | Iteration 50/228 | Training Loss: 1.083\n",
      "Epoch 24/25 | Iteration 100/228 | Training Loss: 1.057\n",
      "Epoch 24/25 | Iteration 150/228 | Training Loss: 1.009\n",
      "Epoch 24/25 | Iteration 200/228 | Training Loss: 1.024\n",
      "Epoch 24/25 | Training Loss: 1.046 | Validation Loss: 1.429\n",
      "\n",
      "Epoch 25/25 | Iteration 50/228 | Training Loss: 1.036\n",
      "Epoch 25/25 | Iteration 100/228 | Training Loss: 1.035\n",
      "Epoch 25/25 | Iteration 150/228 | Training Loss: 1.042\n",
      "Epoch 25/25 | Iteration 200/228 | Training Loss: 1.054\n",
      "Epoch 25/25 | Training Loss: 1.044 | Validation Loss: 1.392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define your device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained ResNet\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-2])  # Remove last fully connected layer and avgpool\n",
    "resnet.to(device)\n",
    "resnet.eval()  # Set ResNet to evaluation mode\n",
    "\n",
    "# Load pre-trained GPT-2\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2.to(device)\n",
    "\n",
    "# ImageCaptioningModel definition\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, resnet, gpt2):\n",
    "        super(ImageCaptioningModel, self).__init__()  # Corrected super call\n",
    "        self.resnet = resnet\n",
    "        self.gpt2 = gpt2\n",
    "        self.proj = nn.Linear(2048, gpt2.config.hidden_size)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask=None):\n",
    "        img_features = self.resnet(images)\n",
    "        img_features = img_features.mean([2, 3])\n",
    "        img_features = self.proj(img_features)\n",
    "        input_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        combined_embeddings = input_embeddings + img_features.unsqueeze(1)\n",
    "        outputs = self.gpt2(inputs_embeds=combined_embeddings, attention_mask=attention_mask, labels=input_ids)\n",
    "        return outputs\n",
    "\n",
    "# Training setup\n",
    "model = ImageCaptioningModel(resnet, gpt2)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 25\n",
    "print_every = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    iteration_loss = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_dataloader, 1):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        images, input_ids, masks = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask=masks)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        iteration_loss += loss.item()\n",
    "        if idx % print_every == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs} | Iteration {idx}/{len(train_dataloader)} | Training Loss: {iteration_loss / print_every:.3f}\")\n",
    "            iteration_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            images, input_ids, masks = batch\n",
    "            outputs = model(images, input_ids, attention_mask=masks)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Training Loss: {total_loss / len(train_dataloader):.3f} | Validation Loss: {val_loss:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d61e281-e460-4aaf-920c-0b65a67d22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'image_captioning_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dae8ea-ff06-4519-a216-8c535bbc822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sukri\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sukri\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.nn as nn\n",
    "import gradio as gr\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the pretrained ResNet model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "resnet.eval()\n",
    "resnet.to(device)\n",
    "\n",
    "# Define the image captioning model\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, resnet, gpt2):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.gpt2 = gpt2\n",
    "        self.proj = nn.Linear(2048, gpt2.config.hidden_size)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask=None):\n",
    "        img_features = self.resnet(images)\n",
    "        img_features = img_features.mean([2, 3])\n",
    "        img_features = self.proj(img_features)\n",
    "        input_embeddings = self.gpt2.transformer.wte(input_ids)\n",
    "        combined_embeddings = input_embeddings + img_features.unsqueeze(1)\n",
    "        outputs = self.gpt2(inputs_embeds=combined_embeddings, attention_mask=attention_mask, labels=input_ids)\n",
    "        return outputs\n",
    "\n",
    "# Initialize the image captioning model and load the state dictionary\n",
    "model = ImageCaptioningModel(resnet, gpt2)\n",
    "model.load_state_dict(torch.load(r\"image_captioning_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Function to generate caption\n",
    "def generate_caption(image, model, tokenizer, max_length=50, temperature=1.0):\n",
    "    caption = [tokenizer.bos_token_id]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_length):\n",
    "            input_ids = torch.LongTensor(caption).unsqueeze(0).to(device)\n",
    "            outputs = model(image.unsqueeze(0), input_ids)\n",
    "            logits = outputs.logits[:, -1, :] / temperature\n",
    "            predicted_id = logits.argmax(1).item()\n",
    "            caption.append(predicted_id)\n",
    "            if predicted_id == tokenizer.eos_token_id and i > 1:\n",
    "                break\n",
    "    generated_caption = tokenizer.decode(caption, skip_special_tokens=True)\n",
    "    return generated_caption\n",
    "\n",
    "# Function to generate caption for an image file\n",
    "def generate_caption_for_image(image_path, model, tokenizer, transform, max_length=50, temperature=1.0):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    caption = generate_caption(image.squeeze(0), model, tokenizer, max_length, temperature)\n",
    "    return caption\n",
    "\n",
    "# Function to create a Gradio interface\n",
    "def main():\n",
    "    def caption_interface(image):\n",
    "        generated_caption = generate_caption_for_image(image, model, tokenizer, transform)\n",
    "        return generated_caption\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=caption_interface,\n",
    "        inputs=gr.Image(type=\"filepath\"),\n",
    "        outputs=gr.Textbox(),\n",
    "        title=\"Image Captioning\",\n",
    "        description=\"Upload an image to generate a caption using a trained model.\"\n",
    "    )\n",
    "\n",
    "    interface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c33fb-b0d4-4b52-bbe8-fa8c431a8cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
